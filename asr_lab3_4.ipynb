{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Labs 3 and 4 &ndash; Viterbi decoding\n",
    "\n",
    "In these labs we'll use the experience &ndash; and code &ndash; you have developed in previous labs to develop your own Viterbi decoder.  You'll want to refer back to [Lecture 5](http://www.inf.ed.ac.uk/teaching/courses/asr/2020-21/asr05-hmm-algorithms.pdf).  Remember that the Viterbi algorithm is used to find the joint probability of an observation sequence $X$ and the **best** path single path $Q$, allowing this best path to be efficiently discovered. \n",
    "\n",
    "Your decoder will find the best path through the WFST representations HMMs that you developed in Labs 1 and 2.  In the case that your WFST is just a linear chain, the best path will provide an *alignment* of the observation sequence to the HMM states.  If your WFST is a set of word or phone loops (or any other structure) then the best path will allow you to recover the most likely transcription for the observed acoustic features, subject to the constraints of your grammar and vocabulary.\n",
    "\n",
    "Observation probabilities $b_j(t)$ will be supplied for you &ndash; you do not need to use the observations $(x_1, \\dotsc x_T)$ directly.  These are supplied via the `observation_model` module.  You can use this as follows:\n",
    "\n",
    "```python\n",
    "import observation_model\n",
    "\n",
    "my_om = observation_model.ObservationModel()\n",
    "\n",
    "my_om.load_audio('filename.wav')  \n",
    "\n",
    "# or use dummy audio for debugging\n",
    "my_om.load_dummy_audio()  # will generate dummy observations as seen in Lab 2, \n",
    "                          # useful for testing\n",
    "    \n",
    "my_om.observation_length()  # returns the sequence length, T  \n",
    "\n",
    "my_om.log_observation_probability(hmm_label, t)  # returns log b_j(t) given HMM label in string form\n",
    "                                                 # raises IndexError if t > T\n",
    "                                                 # raises KeyError if hmm_label is not known\n",
    "```\n",
    "\n",
    "\n",
    "It's easiest to write your decoder as a Python class, and we will supply a template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partner = \"s1922482\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openfst_python as fst\n",
    "from subprocess import check_call\n",
    "from IPython.display import Image\n",
    "import math\n",
    "import random\n",
    " \n",
    "\n",
    "def parse_lexicon(lex_file):\n",
    "    \"\"\"\n",
    "    Parse the lexicon file and return it in dictionary form.\n",
    "\n",
    "    Args:\n",
    "        lex_file (str): filename of lexicon file with structure '<word> <phone1> <phone2>...'\n",
    "                        eg. peppers p eh p er z\n",
    " \n",
    "    Returns:\n",
    "        lex (dict): dictionary mapping words to list of phones\n",
    "    \"\"\"\n",
    "\n",
    "    lex = {}  # create a dictionary for the lexicon entries (this could be a problem with larger lexica)\n",
    "    with open(lex_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.split()  # split at each space\n",
    "            lex[line[0]] = line[1:]  # first field the word, the rest is the phones\n",
    "    return lex\n",
    " \n",
    "lex = parse_lexicon('lexicon.txt')\n",
    " \n",
    "\n",
    "def generate_symbol_tables(lexicon, n=3):\n",
    "    '''\n",
    "    Return word, phone and state symbol tables based on the supplied lexicon\n",
    "\n",
    "    Args:\n",
    "        lexicon (dict): lexicon to use, created from the parse_lexicon() function\n",
    "        n (int): number of states for each phone HMM\n",
    "\n",
    "    Returns:\n",
    "        word_table (fst.SymbolTable): table of words\n",
    "        phone_table (fst.SymbolTable): table of phones\n",
    "        state_table (fst.SymbolTable): table of HMM phone-state IDs\n",
    "    '''\n",
    "    state_table = fst.SymbolTable()\n",
    "    phone_table = fst.SymbolTable()\n",
    "    word_table = fst.SymbolTable()\n",
    "    state_table.add_symbol('<eps>') \n",
    "    phone_table.add_symbol('<eps>') \n",
    "    word_table.add_symbol('<eps>') \n",
    "\n",
    "    for word, phones in lexicon.items():\n",
    "        word_table.add_symbol(word)\n",
    "        for phone in phones:\n",
    "            phone_table.add_symbol(phone)\n",
    "            for i in range(1, n+1):\n",
    "                state_table.add_symbol('{}_{}'.format(phone,i))\n",
    " \n",
    "    return word_table, phone_table, state_table\n",
    " \n",
    "word_table, phone_table, state_table = generate_symbol_tables(lex)\n",
    " \n",
    "\n",
    "def generate_phone_wfst(f, start_state, phone, n):\n",
    "    \"\"\"\n",
    "    Generate a WFST representing an n-state left-to-right phone HMM.\n",
    "\n",
    "    Args:\n",
    "        f (fst.Fst()): an FST object, assumed to exist already\n",
    "        start_state (int): the index of the first state, assumed to exist already\n",
    "        phone (str): the phone label \n",
    "        n (int): number of states of the HMM excluding start and end\n",
    "\n",
    "    Returns:\n",
    "        the final state of the FST\n",
    "    \"\"\"\n",
    "\n",
    "    current_state = start_state\n",
    "    out_label = phone_table.find(phone)\n",
    "    weight1 = fst.Weight('log', -math.log(0.1))\n",
    "    weight2 = fst.Weight('log', -math.log(0.9))\n",
    "\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "\n",
    "        in_label = state_table.find('{}_{}'.format(phone, i))\n",
    "        f.add_arc(current_state, fst.Arc(in_label, 0, weight1, current_state))\n",
    "        new_state = f.add_state()\n",
    "        if i == n:\n",
    "            f.add_arc(current_state, fst.Arc(in_label, out_label, weight2, new_state))\n",
    "        else:\n",
    "            f.add_arc(current_state, fst.Arc(in_label, 0, weight2, new_state))\n",
    "        current_state = new_state\n",
    " \n",
    "    \n",
    "    return current_state\n",
    " \n",
    "def generate_word_wfst(f, start_state, word, n):\n",
    "    \"\"\" Generate a WFST for any word in the lexicon, composed of n-state phone WFSTs.\n",
    "        This will currently output phone labels.  \n",
    " \n",
    "    Args:\n",
    "        f (fst.Fst()): an FST object, assumed to exist already\n",
    "        start_state (int): the index of the first state, assumed to exist already\n",
    "        word (str): the word to generate\n",
    "        n (int): states per phone HMM\n",
    "\n",
    "    Returns:\n",
    "        the constructed WFST\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "    current_state = start_state\n",
    "\n",
    "    phones = lex[word]\n",
    "    for phone in phones:\n",
    "        current_state = generate_phone_wfst(f, current_state, phone, n)\n",
    "    f.set_final(current_state)\n",
    "\n",
    "    return f\n",
    " \n",
    "\n",
    "f = fst.Fst('log')\n",
    "start = f.add_state()\n",
    "f.set_start(start)\n",
    " \n",
    "generate_word_wfst(f, start, 'peppers', 3)\n",
    "f.set_input_symbols(state_table)\n",
    "f.set_output_symbols(phone_table)\n",
    "# f.draw('tmp.dot', portrait=True)\n",
    "# check_call(['dot','-Tpng', '-Gdpi=200','tmp.dot','-o','tmp.png'])\n",
    "# Image(filename='tmp.png')\n",
    "\n",
    "for state in f.states():\n",
    "    # iterate over all arcs leaving this state    \n",
    "    for arc in f.arcs(state):\n",
    "         print(state, arc.ilabel, arc.olabel, arc.weight, arc.nextstate)\n",
    "\n",
    "def traverse_arcs(state):\n",
    "    \"\"\"Traverse every arc leaving a particular state\n",
    "    \"\"\"\n",
    "    for arc in f.arcs(state):\n",
    "        print(state, arc.ilabel, arc.olabel, arc.weight, arc.nextstate)\n",
    "        \n",
    "        if arc.nextstate != state:   # don't follow the self-loops or we'll get stuck forever!\n",
    "            traverse_arcs(arc.nextstate)\n",
    "\n",
    "s = f.start()\n",
    "traverse_arcs(s)\n",
    "\n",
    "def sample_random_path(f):\n",
    "    '''Given an FST, randomly sample a path through it.\n",
    "    \n",
    "        Args:\n",
    "            f (fst.Fst()): an FST\n",
    "        \n",
    "        Returns:\n",
    "            input_label_seq (list(str)): the list of input labels from the arcs that were sampled\n",
    "            output_label_seq (list(str)): the list of output labels from the arcs that were sampled\n",
    "        '''\n",
    "    curr_state = f.start() # start from beginning\n",
    "    weight_type = f.weight_type() # type of weights used in the fst\n",
    "    input_label_seq = []\n",
    "    output_label_seq = []\n",
    "\n",
    "    while f.final(curr_state) == fst.Weight(weight_type, 'inf'): # the .final method returns the probability of a state being final\n",
    "                                                                 # it's infinite when the state is NOT final\n",
    "            arc_list = list(f.arcs(curr_state))\n",
    "            sampled_arc = random.sample(arc_list, 1)[0]\n",
    "            input_label_seq.append(state_table.find(sampled_arc.ilabel))\n",
    "            output_label_seq.append(state_table.find(sampled_arc.olabel))\n",
    "            \n",
    "            curr_state = sampled_arc.nextstate\n",
    "            \n",
    "    return input_label_seq, output_label_seq\n",
    "\n",
    "input_label_seq, output_label_seq = sample_random_path(f)\n",
    "\n",
    "print('\\n'.join(['{} {}'.format(input_label_seq[i], output_label_seq[i]) for i in range(len(input_label_seq))]))\n",
    "\n",
    "def sample_random_path_prob(f):\n",
    "    '''Given an FST, randomly sample a path through it and compute the negative log probability.\n",
    "    \n",
    "        Args:\n",
    "            f (fst.Fst()): an FST\n",
    "        \n",
    "        Returns:\n",
    "            input_label_seq (list(str)): the list of input labels from the arcs that were sampled\n",
    "            output_label_seq (list(str)): the list of output labels from the arcs that were sampled\n",
    "            neg_log_prob (float): negative log probability of the sampled path\n",
    "        '''\n",
    "    curr_state = f.start() # start from beginning\n",
    "    weight_type = f.weight_type() # type of weights used in the fst\n",
    "    input_label_seq = []\n",
    "    output_label_seq = []\n",
    "    neg_log_prob = 0.0\n",
    "\n",
    "    while f.final(curr_state) == fst.Weight(weight_type, 'inf'): # the .final method returns the probability of a state being final\n",
    "                                                                 # it's infinite when the state is NOT final\n",
    "            \n",
    "            # your code here\n",
    "            arc_list = list(f.arcs(curr_state))\n",
    "            sampled_arc = random.sample(arc_list, 1)[0]\n",
    "            input_label_seq.append(state_table.find(sampled_arc.ilabel))\n",
    "            output_label_seq.append(state_table.find(sampled_arc.olabel))\n",
    "            \n",
    "            neg_log_prob += float(sampled_arc.weight)\n",
    "            \n",
    "            curr_state = sampled_arc.nextstate\n",
    "            \n",
    "    return input_label_seq, output_label_seq, -neg_log_prob\n",
    "\n",
    "input_label_seq, output_label_seq, neg_log_prob = sample_random_path_prob(f)\n",
    "\n",
    "print('\\n'.join(['{} {}'.format(input_label_seq[i], output_label_seq[i]) for i in range(len(input_label_seq))]))\n",
    "print(neg_log_prob)\n",
    "\n",
    "def observation_probability(hmm_label, t):\n",
    "    \"\"\" Computes b_j(t) where j is the current state\n",
    "    \n",
    "    This is just a dummy version!  In later labs we'll generate \n",
    "    probabilities for real speech frames.\n",
    "    \n",
    "    You don't need to look at this function in detail.\n",
    "    \n",
    "    Args: hmm_label (str): the HMM state label, j.  We'll use string form: \"p_1\", \"p_2\", \"eh_1\" etc  \n",
    "          t (int) : current time step, starting at 1\n",
    "          \n",
    "    Returns: \n",
    "          p (float): the observation probability p(x_t | q_t = hmm_label)\n",
    "    \"\"\"\n",
    "    \n",
    "    p = {} # dictionary of probabilities\n",
    "    \n",
    "    assert(t>0)\n",
    "    \n",
    "    # this is just a simulation!\n",
    "    if t < 4:\n",
    "        p = {'p_1': 1.0, 'p_2':1.0, 'p_3': 1.0, 'eh_1':0.2}\n",
    "    elif t < 9:\n",
    "        p = {'p_3': 0.5, 'eh_1':1.0, 'eh_2': 1.0, 'eh_3': 1.0}\n",
    "    elif t < 13:\n",
    "        p = {'eh_3': 1.0, 'p_1': 1.0, 'p_2': 1.0, 'p_3':1.0, 'er_1':0.5}\n",
    "    elif t < 18:\n",
    "        p = {'p_3': 1.0, 'er_1': 1.0, 'er_2': 1.0, 'er_3':0.7}\n",
    "    elif t < 25:\n",
    "        p = {'er_3': 1.0, 'z_1': 1.0, 'z_2': 1.0, 'z_3':1.0}\n",
    "    else:\n",
    "        p = {'z_2': 0.5, 'z_3': 1.0}\n",
    "        \n",
    "    for hmm_label1 in ['p_1', 'p_2', 'p_3', 'eh_1', 'eh_2', 'eh_3', 'er_1', 'er_2', 'er_3', 'z_1', 'z_2', 'z_3']:        \n",
    "        if hmm_label1 not in p:\n",
    "            p[hmm_label1] = 0.01  # give all other states a small probability to avoid zero probability\n",
    "            \n",
    "    # normalise the probabilities:\n",
    "    scale = sum(p.values())\n",
    "    for k in p:\n",
    "        p[k] = p[k]/scale\n",
    "        \n",
    "    return p[hmm_label1]\n",
    "\n",
    "def sample_random_path_obs_prob(f):\n",
    "    '''Given an FST and observation probabilities, randomly sample a path\n",
    "        through it and compute the negative log probability.\n",
    "    \n",
    "        Args:\n",
    "            f (fst.Fst()): an FST\n",
    "        \n",
    "        Returns:\n",
    "            input_label_seq (list(str)): the list of input labels from the arcs that were sampled\n",
    "            output_label_seq (list(str)): the list of output labels from the arcs that were sampled\n",
    "            neg_log_prob (float): negative log probability of the sampled path\n",
    "        '''\n",
    "    t = 1\n",
    "    curr_state = f.start() # start from beginning\n",
    "    weight_type = f.weight_type() # type of weights used in the fst\n",
    "    input_label_seq = []\n",
    "    output_label_seq = []\n",
    "    neg_log_prob = 0.0 # log(1) = 0\n",
    "\n",
    "    while f.final(curr_state) == fst.Weight(weight_type, 'inf'):\n",
    "        arc_list = list(f.arcs(curr_state))\n",
    "        sampled_arc = random.sample(arc_list, 1)[0]\n",
    "        input_label_seq.append(state_table.find(sampled_arc.ilabel))\n",
    "        output_label_seq.append(state_table.find(sampled_arc.olabel))\n",
    "            \n",
    "        neg_log_prob += float(sampled_arc.weight) - math.log(observation_probability(state_table.find(sampled_arc.ilabel), t))\n",
    "        \n",
    "        t += 1\n",
    "        curr_state = sampled_arc.nextstate\n",
    "    \n",
    "    return input_label_seq, output_label_seq, neg_log_prob\n",
    "\n",
    "input_label_seq, output_label_seq, neg_log_prob = sample_random_path_obs_prob(f)\n",
    "print('\\n'.join(['{} {}'.format(input_label_seq[i], output_label_seq[i]) for i in range(len(input_label_seq))]))\n",
    "print(neg_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import observation_model\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class MyViterbiDecoder:\n",
    "    \n",
    "    NLL_ZERO = 1e10  # define a constant representing -log(0).  This is really infinite, but approximate\n",
    "                     # it here with a very large number\n",
    "    \n",
    "    def __init__(self, f, audio_file_name):\n",
    "        \"\"\"Set up the decoder class with an audio file and WFST f\n",
    "        \"\"\"\n",
    "        self.om = observation_model.ObservationModel()\n",
    "        self.f = f\n",
    "        \n",
    "        if audio_file_name:\n",
    "            self.om.load_audio(audio_file_name)\n",
    "        else:\n",
    "            self.om.load_dummy_audio()\n",
    "        \n",
    "        self.initialise_decoding()\n",
    "    \n",
    "    def initialise_decoding(self):\n",
    "        \"\"\"set up the values for V_j(0) (as negative log-likelihoods)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.V = []\n",
    "        for t in range(self.om.observation_length()+1):\n",
    "            self.V.append([self.NLL_ZERO]*self.f.num_states())\n",
    "        \n",
    "        # The above code means that self.V[t][j] for t = 0, ... T gives the Viterbi cost\n",
    "        # of state j, time t (in negative log-likelihood form)\n",
    "        # Initialising the costs to NLL_ZERO effectively means zero probability    \n",
    "        \n",
    "        # give the WFST start state a probability of 1.0   (NLL = 0.0)\n",
    "        self.V[0][f.start()] = 0.0\n",
    "        \n",
    "        # some WFSTs might have arcs with epsilon on the input (you might have already created \n",
    "        # examples of these in earlier labs) these correspond to non-emitting states, \n",
    "        # which means that we need to process them without stepping forward in time.  \n",
    "        # Don't worry too much about this!  \n",
    "        self.traverse_epsilon_arcs(0)\n",
    "        \n",
    "        self.B_j = []\n",
    "        \n",
    "    def traverse_epsilon_arcs(self, t):\n",
    "        \"\"\"Traverse arcs with <eps> on the input at time t\n",
    "        \n",
    "        These correspond to transitions that don't emit an observation\n",
    "        \n",
    "        We've implemented this function for you as it's slightly trickier than\n",
    "        the normal case.  You might like to look at it to see what's going on, but\n",
    "        don't worry if you can't fully follow it.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        states_to_traverse = list(range(self.f.num_states())) # traverse all states\n",
    "        while states_to_traverse:\n",
    "            \n",
    "            # Set i to the ID of the current state, the first \n",
    "            # item in the list (and remove it from the list)\n",
    "            i = states_to_traverse.pop(0)   \n",
    "        \n",
    "            # don't bother traversing states which have zero probability\n",
    "            if self.V[t][i] == self.NLL_ZERO:\n",
    "                    continue\n",
    "        \n",
    "            for arc in self.f.arcs(i):\n",
    "                \n",
    "                if arc.ilabel == 0:     # if <eps> transition\n",
    "                  \n",
    "                    j = arc.nextstate   # ID of next state  \n",
    "                \n",
    "                    if self.V[t][j] > self.V[t][i] + float(arc.weight):\n",
    "                        \n",
    "                        # this means we've found a lower-cost path to\n",
    "                        # state j at time t.  We might need to add it\n",
    "                        # back to the processing queue.\n",
    "                        self.V[t][j] = self.V[t][i] + float(arc.weight)\n",
    "                  \n",
    "                        if j not in states_to_traverse:\n",
    "                            states_to_traverse.append(j)\n",
    "\n",
    "    \n",
    "    def forward_step(self, t):\n",
    "        # RECURSION: Slide 11\n",
    "        # V_j(t) = max_i V_i(t-1) * a_{ij} * b_j(x_t)\n",
    "        # B_j(t) = argmax_i V_i(t-1) * a_{ij} * b_j(x_t)\n",
    "        for j in range(self.f.num_states()):\n",
    "            prevs = []\n",
    "            for i in range(self.f.num_states()):\n",
    "                a_ij = self.NLL_ZERO\n",
    "                b_jt = self.NLL_ZERO\n",
    "                for arc in self.f.arcs(i):\n",
    "                    if arc.nextstate == j:\n",
    "                        a_ij = float(arc.weight)\n",
    "                        hmm_label = state_table.find(arc.ilabel)\n",
    "                        b_jt = self.om.log_observation_probability(hmm_label, t)\n",
    "                prevs.append(self.V[t-1][i] + a_ij + b_jt)\n",
    "            state = np.argmax(prevs)\n",
    "            self.B_j.append(state)\n",
    "            self.V[t][j] = max(prevs)\n",
    "    \n",
    "    def finalise_decoding(self):\n",
    "        # TERMINATION: Slide 11\n",
    "        self.V_E = np.max(self.V[-1])\n",
    "        self.B_E = np.argmax(self.V[-1])\n",
    "    \n",
    "    def decode(self):\n",
    "        \n",
    "        self.initialise_decoding()\n",
    "        t = 1\n",
    "        while t <= self.om.observation_length():\n",
    "            self.forward_step(t)\n",
    "            self.traverse_epsilon_arcs(t)\n",
    "            t += 1\n",
    "        \n",
    "        self.finalise_decoding()\n",
    "    \n",
    "    def backtrace(self):\n",
    "        \n",
    "        # TODO - exercise \n",
    "        \n",
    "        # complete code to trace back through the\n",
    "        # best state sequence\n",
    "        \n",
    "        # You'll need to create a structure B_j(t) to store the \n",
    "        # back-pointers (see lectures), and amend the functions above to fill it.\n",
    "        return self.B_j # maybe reversed(self.B_j)?\n",
    "        \n",
    "\n",
    "# to call the decoder (in a dummy example)\n",
    "# f will be a WFST that you have created in a previous lab\n",
    "decoder = MyViterbiDecoder(f, '')   # empty string '' just means use dummy probabilities for testing\n",
    "decoder.decode()\n",
    "print(decoder.backtrace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises &ndash; Lab 3\n",
    "\n",
    "The `__init__()`, `initialise_decoding()` and `decode()` functions have been completed in the template above.\n",
    "You should aim to complete the `forward_step()` and `finalise_decoding()` functions for Lab 3.  Don't worry about implementing the back-trace in Lab 3.\n",
    "\n",
    "You should draw on your solutions to Lab 2 &ndash; the main difference now is that, rather than simply sampling a single path and computing its likelihood, you'll need to compute and store, at every time step $t$, and for every state in the WFST, the likelihood of the best path reaching that state after $t$ time steps.  For how to do this using the Viterbi algorithm, see Lecture 5, slides 11 onwards.\n",
    "\n",
    "Test your algorithms on an WFST that recognises the word \"*pepper*\" and one that recognises any word in the vocabulary.\n",
    "\n",
    "\n",
    "## Exercises &ndash; Lab 4\n",
    "\n",
    "Now complete the `backtrace()` function to allow the best path to be recovered.  As noted in the code, you'll need to create a structure to store $B_j(t)$, which stores the identity of the best preceding state reaching state $j$ at time $t$ .  You can follow a similar method to storing $V_j(t)$, given in the code already.  You'll need to add it to the `initialise_decoding()`, `forward_step()` and `finalise_decoding()` functions.\n",
    "\n",
    "Once you are happy that your function works, you should amend your code so that you can also recover the sequence of *output* symbols on your WFST's best path as well.  This should allow you to produce your first word-recognition result!\n",
    "\n",
    "### Working on real speech\n",
    "\n",
    "You can now find the first batch of recordings made by ASR students in `/group/teaching/asr/labs/recordings`.  You can test your decoder on real speech data by passing the full path to the WAV file when you create your `MyViterbiDecoder` object.  The transcriptions are also available in the same folder.\n",
    "\n",
    "When working with real speech, you may want to modify your code to print only the word output labels! \n",
    "\n",
    "If you are interested, the observation model (kindly supplied by Andrea) is a monophone time-delay neural network trained using the lattice-free MMI criterion, on the WSJ corpus of read speech.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
